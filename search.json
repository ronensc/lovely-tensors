[
  {
    "objectID": "repr_plt.html",
    "href": "repr_plt.html",
    "title": "ðŸ“Š View as a histogram",
    "section": "",
    "text": "source\n\nplot\n\n plot (t:torch.Tensor, center='zero', max_s=10000, plt0=True, fmt='png',\n       ax=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nTensor\n\nTensor to explore\n\n\ncenter\nstr\nzero\nCenter plot on zero, mean, or range\n\n\nmax_s\nint\n10000\nDraw up to this many samples. =0 to draw all\n\n\nplt0\nbool\nTrue\nTake zero values into account\n\n\nfmt\nstr\npng\nRender figure in this format (png, svg)\n\n\nax\nNoneType\nNone\nOptionally supply your own matplotlib axes.\n\n\n\n\ntorch.manual_seed(42)\nt = torch.randn(100000)+3\nplot(t)\n\n\n\n\n\nplot(t, center=\"range\")\n\n\n\n\n\nplot(t, center=\"mean\")\n\n\n\n\n\nplot(torch.nn.functional.relu(t-3))\n\n\n\n\n\nplot(torch.nn.functional.relu(t-3), plt0=False)\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 2))\n_ = plot(t, ax=ax)"
  },
  {
    "objectID": "history.html",
    "href": "history.html",
    "title": "ðŸ“œ IPythonâ€™s history obsession",
    "section": "",
    "text": "Letâ€™s have a look at what happens when a variable falls of the end of a cell.\n\ntorch.cuda.memory_allocated()\n\n0\n\n\n\nt = torch.tensor(10, device=\"cuda\")\nt\n\ntensor(10, device='cuda:0')\n\n\n\ntorch.cuda.memory_allocated()\n\n512\n\n\n\ndel t\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()\n\n512\n\n\nAbove, I allocated a tensor in CUDA memory and displayed it as the cell output, then deleted it.\nI did not use Lovely Tensors, just plain PyTorch.\nWhy is the CUDA memory not freed? Is there still a reference to the tensor somewhere?\n\nYes.\n\n\ndir()[:10] # Global variables\n\n['In',\n 'Out',\n '_',\n '_2',\n '_3',\n '_4',\n '_5',\n '_VSCode_matplotLib_FigureFormats',\n '__',\n '___']\n\n\nDo you see the _ variables?\nThey are created by IPython and every cell output you run is saved:\nhttps://ipython.readthedocs.io/en/stable/interactive/reference.html#output-caching-system\n\nprint(_3) # Here is my tensor from cell 3\n\ntensor(10, device='cuda:0')\n\n\nIf this is not the behavior you want, you can disable it by adding\n%config ZMQInteractiveShell.cache_size = 0\nat the begining of your notebook, but I think this only works in plain Jupyter and not Jupyter in vscode.\nAlternatively, find your pytorch config file (for me itâ€™s ~/.ipython/profile_default/ipython_kernel_config.py)\nand set ZMQInteractiveShell.cache_size to 0."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Install",
    "text": "Install\npip install lovely-tensors"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "How to use",
    "text": "How to use\nHow often do you find yourself debugging PyTorch code? You dump a tensor to the cell output, and see this:\n\nnumbers\n\ntensor([[[-0.3541, -0.3369, -0.4054,  ..., -0.5596, -0.4739,  2.2489],\n         [-0.4054, -0.4226, -0.4911,  ..., -0.9192, -0.8507,  2.1633],\n         [-0.4739, -0.4739, -0.5424,  ..., -1.0390, -1.0390,  2.1975],\n         ...,\n         [-0.9020, -0.8335, -0.9363,  ..., -1.4672, -1.2959,  2.2318],\n         [-0.8507, -0.7822, -0.9363,  ..., -1.6042, -1.5014,  2.1804],\n         [-0.8335, -0.8164, -0.9705,  ..., -1.6555, -1.5528,  2.1119]],\n\n        [[-0.1975, -0.1975, -0.3025,  ..., -0.4776, -0.3725,  2.4111],\n         [-0.2500, -0.2325, -0.3375,  ..., -0.7052, -0.6702,  2.3585],\n         [-0.3025, -0.2850, -0.3901,  ..., -0.7402, -0.8102,  2.3761],\n         ...,\n         [-0.4251, -0.2325, -0.3725,  ..., -1.0903, -1.0203,  2.4286],\n         [-0.3901, -0.2325, -0.4251,  ..., -1.2304, -1.2304,  2.4111],\n         [-0.4076, -0.2850, -0.4776,  ..., -1.2829, -1.2829,  2.3410]],\n\n        [[-0.6715, -0.9853, -0.8807,  ..., -0.9678, -0.6890,  2.3960],\n         [-0.7238, -1.0724, -0.9678,  ..., -1.2467, -1.0201,  2.3263],\n         [-0.8284, -1.1247, -1.0201,  ..., -1.2641, -1.1596,  2.3786],\n         ...,\n         [-1.2293, -1.4733, -1.3861,  ..., -1.5081, -1.2641,  2.5180],\n         [-1.1944, -1.4559, -1.4210,  ..., -1.6476, -1.4733,  2.4308],\n         [-1.2293, -1.5256, -1.5081,  ..., -1.6824, -1.5256,  2.3611]]])\n\n\nWas it really useful for you, as a human, to see all these numbers?\nWhat is the shape? The size?\nWhat are the statistics?\nAre any of the values nan or inf?\nIs it an image of a man holding a tench?\n\nimport lovely_tensors as lt\n\n\nlt.monkey_patch()"
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Summary",
    "text": "Summary\n\nnumbers # torch.Tensor\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073\n\n\nBetter, huh?\n\nnumbers[1,:6,1] # Still shows values if there are not too many.\n\ntensor[6] xâˆˆ[-0.443, -0.197] Î¼=-0.311 Ïƒ=0.091 [-0.197, -0.232, -0.285, -0.373, -0.443, -0.338]\n\n\n\nspicy = numbers[0,:12,0].clone()\n\nspicy[0] *= 10000\nspicy[1] /= 10000\nspicy[2] = float('inf')\nspicy[3] = float('-inf')\nspicy[4] = float('nan')\n\nspicy = spicy.reshape((2,6))\nspicy # Spicy stuff\n\n\ntensor[2, 6] n=12 xâˆˆ[-3.541e+03, -4.054e-05] Î¼=-393.842 Ïƒ=1.180e+03 +Inf! -Inf! NaN!\n\n\n\n\ntorch.zeros(10, 10) # A zero tensor - make it obvious\n\n\ntensor[10, 10] all_zeros\n\n\n\n\nspicy.v # Verbose\n\n\ntensor[2, 6] n=12 xâˆˆ[-3.541e+03, -4.054e-05] Î¼=-393.842 Ïƒ=1.180e+03 +Inf! -Inf! NaN!\ntensor([[-3.5405e+03, -4.0543e-05,         inf,        -inf,         nan, -6.1093e-01],\n        [-6.1093e-01, -5.9380e-01, -5.9380e-01, -5.4243e-01, -5.4243e-01, -5.4243e-01]])\n\n\n\n\nspicy.p # The plain old way\n\ntensor([[-3.5405e+03, -4.0543e-05,         inf,        -inf,         nan, -6.1093e-01],\n        [-6.1093e-01, -5.9380e-01, -5.9380e-01, -5.4243e-01, -5.4243e-01, -5.4243e-01]])"
  },
  {
    "objectID": "index.html#going-.deeper",
    "href": "index.html#going-.deeper",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Going .deeper",
    "text": "Going .deeper\n\nnumbers.deeper\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073\n  tensor[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036\n  tensor[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973\n  tensor[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178\n\n\n\n# You can go deeper if you need to\nnumbers[:,:3,:5].deeper(2)\n\ntensor[3, 3, 5] n=45 xâˆˆ[-1.316, -0.197] Î¼=-0.593 Ïƒ=0.306\n  tensor[3, 5] n=15 xâˆˆ[-0.765, -0.337] Î¼=-0.492 Ïƒ=0.124\n    tensor[5] xâˆˆ[-0.440, -0.337] Î¼=-0.385 Ïƒ=0.041 [-0.354, -0.337, -0.405, -0.440, -0.388]\n    tensor[5] xâˆˆ[-0.662, -0.405] Î¼=-0.512 Ïƒ=0.108 [-0.405, -0.423, -0.491, -0.577, -0.662]\n    tensor[5] xâˆˆ[-0.765, -0.474] Î¼=-0.580 Ïƒ=0.125 [-0.474, -0.474, -0.542, -0.645, -0.765]\n  tensor[3, 5] n=15 xâˆˆ[-0.513, -0.197] Î¼=-0.321 Ïƒ=0.099\n    tensor[5] xâˆˆ[-0.303, -0.197] Î¼=-0.243 Ïƒ=0.055 [-0.197, -0.197, -0.303, -0.303, -0.215]\n    tensor[5] xâˆˆ[-0.408, -0.232] Î¼=-0.327 Ïƒ=0.084 [-0.250, -0.232, -0.338, -0.408, -0.408]\n    tensor[5] xâˆˆ[-0.513, -0.285] Î¼=-0.394 Ïƒ=0.102 [-0.303, -0.285, -0.390, -0.478, -0.513]\n  tensor[3, 5] n=15 xâˆˆ[-1.316, -0.672] Î¼=-0.964 Ïƒ=0.176\n    tensor[5] xâˆˆ[-0.985, -0.672] Î¼=-0.846 Ïƒ=0.123 [-0.672, -0.985, -0.881, -0.776, -0.916]\n    tensor[5] xâˆˆ[-1.212, -0.724] Î¼=-0.989 Ïƒ=0.179 [-0.724, -1.072, -0.968, -0.968, -1.212]\n    tensor[5] xâˆˆ[-1.316, -0.828] Î¼=-1.058 Ïƒ=0.179 [-0.828, -1.125, -1.020, -1.003, -1.316]"
  },
  {
    "objectID": "index.html#now-in-.rgb-color",
    "href": "index.html#now-in-.rgb-color",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Now in .rgb color",
    "text": "Now in .rgb color\nThe important queston - is it our man?\n\nnumbers.rgb\n\n\n\n\nMaaaaybe? Looks like someone normalized him.\n\nin_stats = ( (0.485, 0.456, 0.406),     # mean \n             (0.229, 0.224, 0.225) )    # std\n\n# numbers.rgb(in_stats, cl=True) # For channel-last input format\nnumbers.rgb(in_stats)\n\n\n\n\nItâ€™s indeed our hero, the Tenchman!"
  },
  {
    "objectID": "index.html#plt-the-statistics",
    "href": "index.html#plt-the-statistics",
    "title": "â¤ï¸ Lovely Tensors",
    "section": ".plt the statistics",
    "text": ".plt the statistics\n\n(numbers+3).plt\n\n\n\n\n\n(numbers+3).plt(center=\"mean\", max_s=1000)\n\n\n\n\n\n(numbers+3).plt(center=\"range\")"
  },
  {
    "objectID": "index.html#see-the-.chans",
    "href": "index.html#see-the-.chans",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "See the .chans",
    "text": "See the .chans\n\n# .chans will map values betwen [-1,1] to colors.\n# Make our values fit into that range to avoid clipping.\nmean = torch.tensor(in_stats[0])[:,None,None]\nstd = torch.tensor(in_stats[1])[:,None,None]\nnumbers_01 = (numbers*std + mean)\nnumbers_01\n\ntensor[3, 196, 196] n=115248 xâˆˆ[0., 1.000] Î¼=0.361 Ïƒ=0.248\n\n\n\nnumbers_01.chans\n\n\n\n\nLetâ€™s try with a Convolutional Neural Network\n\nfrom torchvision.models import vgg11\n\n\nfeatures: torch.nn.Sequential = vgg11().features\n\n# I saved the first 5 layers in \"features.pt\"\n_ = features.load_state_dict(torch.load(\"../features.pt\"), strict=False)\n\n\n# Activatons of the second max pool layer of VGG11\nacts = (features[:6](numbers[None])[0]/2) # /2 to reduce clipping\nacts\n\ntensor[128, 49, 49] n=307328 xâˆˆ[0., 12.508] Î¼=0.367 Ïƒ=0.634 grad DivBackward0\n\n\n\nacts[:4].chans(cmap=\"coolwarm\", scale=4)"
  },
  {
    "objectID": "index.html#grouping",
    "href": "index.html#grouping",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Grouping",
    "text": "Grouping\n\n# Make 8 images with progressively higher brightness and stack them 2x2x2.\neight_images = (torch.stack([numbers]*8)\n                    .add(torch.linspace(-3, 3, 8)[:,None,None,None])\n                    .mul(torch.tensor(in_stats[1])[:,None,None])\n                    .add(torch.tensor(in_stats[0])[:,None,None])\n                    .clamp(0,1)\n                    .view(2,2,2,3,196,196)\n)\neight_images\n\ntensor[2, 2, 2, 3, 196, 196] n=921984 xâˆˆ[0., 1.000] Î¼=0.411 Ïƒ=0.369\n\n\n\neight_images.rgb\n\n\n\n\n\n# Weights of the second conv layer of VGG11\nfeatures[3].weight\n\nParameter containing:\nParameter[128, 64, 3, 3] n=73728 xâˆˆ[-0.783, 0.776] Î¼=-0.004 Ïƒ=0.065 grad\n\n\nI want +/- 2Ïƒ to fall in the range [-1..1]\n\nweights = features[3].weight.data\nweights = weights / (2*2*weights.std()) # *2 because we want 2Ïƒ on both sides, so 4Ïƒ\n# weights += weights.std() * 2\nweights.plt\n\n\n\n\n\n# Weights of the second conv layer (64ch -> 128ch) of VGG11,\n# grouped per output channel.\nweights.chans(frame_px=1, gutter_px=0)\n\n\n\n\nItâ€™s a bit hard to see. Scale up 10x, but onyl show the first 4 filters.\n\nweights[:4].chans(frame_px=1, gutter_px=0, scale=10)"
  },
  {
    "objectID": "index.html#options",
    "href": "index.html#options",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Options",
    "text": "Options\nSee docs for more\n\nfrom lovely_tensors import set_config, config, lovely, get_config\n\n\nset_config(precision=5, sci_mode=True, color=False)\ntorch.tensor([1, 2, torch.nan])\n\ntensor[3] Î¼=1.50000e+00 Ïƒ=7.07107e-01 NaN! [1.00000e+00, 2.00000e+00, nan]\n\n\n\nset_config(precision=None, sci_mode=None, color=None) # None -> Reset to defaults\n\n\nprint(torch.tensor([1., 2]))\n# Or with config context manager.\nwith config(sci_mode=True, precision=5):\n    print(torch.tensor([1., 2]))\n\nprint(torch.tensor([1., 2]))\n\ntensor[2] Î¼=1.500 Ïƒ=0.707 [1.000, 2.000]\ntensor[2] Î¼=1.50000e+00 Ïƒ=7.07107e-01 [1.00000e+00, 2.00000e+00]\ntensor[2] Î¼=1.500 Ïƒ=0.707 [1.000, 2.000]"
  },
  {
    "objectID": "index.html#without-.monkey_patch",
    "href": "index.html#without-.monkey_patch",
    "title": "â¤ï¸ Lovely Tensors",
    "section": "Without .monkey_patch",
    "text": "Without .monkey_patch\n\nlt.lovely(spicy)\n\n\ntensor[2, 6] n=12 xâˆˆ[-3.541e+03, -4.054e-05] Î¼=-393.842 Ïƒ=1.180e+03 +Inf! -Inf! NaN!\n\n\n\n\nlt.lovely(spicy, verbose=True)\n\n\ntensor[2, 6] n=12 xâˆˆ[-3.541e+03, -4.054e-05] Î¼=-393.842 Ïƒ=1.180e+03 +Inf! -Inf! NaN!\ntensor([[-3.5405e+03, -4.0543e-05,         inf,        -inf,         nan, -6.1093e-01],\n        [-6.1093e-01, -5.9380e-01, -5.9380e-01, -5.4243e-01, -5.4243e-01, -5.4243e-01]])\n\n\n\n\nlt.lovely(numbers, depth=1)\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073\n  tensor[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036\n  tensor[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973\n  tensor[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178\n\n\n\nlt.rgb(numbers, in_stats)\n\n\n\n\n\nlt.plot(numbers, center=\"mean\")\n\n\n\n\n\nlt.chans(numbers_01)"
  },
  {
    "objectID": "repr_str.html",
    "href": "repr_str.html",
    "title": "ðŸ§¾ View as a summary",
    "section": "",
    "text": "nasties = randoms[:12].clone()\n\nnasties[0] *= 10000\nnasties[1] /= 10000\nnasties[3] = float('inf')\nnasties[4] = float('-inf')\nnasties[5] = float('nan')\nnasties = nasties.reshape((2,6))\n\n\nsource\n\nlovely\n\n lovely (t:torch.Tensor, verbose=False, plain=False, depth=0, color=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nTensor\n\nTensor of interest\n\n\nverbose\nbool\nFalse\nWhether to show the full tensor\n\n\nplain\nbool\nFalse\nJust print if exactly as before\n\n\ndepth\nint\n0\nShow stats in depth\n\n\ncolor\nNoneType\nNone\nForce color (True/False) or auto.\n\n\n\n\n\nExamples\n\nprint(lovely(randoms[0]))\nprint(lovely(randoms[:2]))\nprint(lovely(randoms[:6].view(2, 3))) # More than 2 elements -> show statistics\nprint(lovely(randoms[:11]))           # More than 10 -> suppress data output\n\ntensor 1.927\ntensor[2] Î¼=1.707 Ïƒ=0.311 [1.927, 1.487]\ntensor[2, 3] n=6 xâˆˆ[-2.106, 1.927] Î¼=0.276 Ïƒ=1.594 [[1.927, 1.487, 0.901], [-2.106, 0.678, -1.235]]\ntensor[11] xâˆˆ[-2.106, 1.927] Î¼=0.046 Ïƒ=1.384\n\n\n\ngrad = torch.tensor(1., requires_grad=True, dtype=torch.float64)\nprint(lovely(grad)); print(lovely(grad+1))\n\ntensor f64 grad 1.000\ntensor f64 grad AddBackward0 2.000\n\n\n\nif torch.cuda.is_available():\n    print(lovely(torch.tensor(1., device=torch.device(\"cuda:0\"))))\n    test_eq(str(lovely(torch.tensor(1., device=torch.device(\"cuda:0\")))), \"tensor cuda:0 1.000\")\n\ntensor cuda:0 1.000\n\n\nDo we have any floating point nasties? Is the tensor all zeros?\n\n# Statistics and range are calculated on good values only, if there are at lest 3 of them.\nlovely(nasties)\n\n\ntensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 +Inf! -Inf! NaN!\n\n\n\n\nlovely(nasties, color=False)\n\ntensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 +Inf! -Inf! NaN!\n\n\n\nlovely(torch.tensor([float(\"nan\")]*11))\n\n\ntensor[11] NaN!\n\n\n\n\nlovely(torch.zeros(12))\n\n\ntensor[12] all_zeros\n\n\n\n\nlovely(torch.randn([0,0,0], dtype=torch.float16))\n\n\ntensor[0, 0, 0] f16 empty\n\n\n\n\nlovely(torch.tensor([1,2,3], dtype=torch.int32))\n\ntensor[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=1.000 [1, 2, 3]\n\n\n\ntorch.set_printoptions(linewidth=120)\nlovely(nasties, verbose=True)\n\n\ntensor[2, 6] n=12 xâˆˆ[-1.605, 1.927e+04] Î¼=2.141e+03 Ïƒ=6.423e+03 +Inf! -Inf! NaN!\ntensor([[ 1.9269e+04,  1.4873e-04,  9.0072e-01,         inf,        -inf,         nan],\n        [-4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00, -3.9248e-01, -1.4036e+00]])\n\n\n\n\nlovely(nasties, plain=True)\n\ntensor([[ 1.9269e+04,  1.4873e-04,  9.0072e-01,         inf,        -inf,         nan],\n        [-4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00, -3.9248e-01, -1.4036e+00]])\n\n\n\nimage = torch.load(\"mysteryman.pt\")\nimage[1,2,3] = float('nan')\n\nlovely(image, depth=2) # Limited by set_config(deeper_lines=N)\n\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073 NaN!\n  tensor[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036\n    tensor[196] xâˆˆ[-1.912, 2.249] Î¼=-0.673 Ïƒ=0.522\n    tensor[196] xâˆˆ[-1.861, 2.163] Î¼=-0.738 Ïƒ=0.418\n    tensor[196] xâˆˆ[-1.758, 2.198] Î¼=-0.806 Ïƒ=0.397\n    tensor[196] xâˆˆ[-1.656, 2.249] Î¼=-0.849 Ïƒ=0.369\n    tensor[196] xâˆˆ[-1.673, 2.198] Î¼=-0.857 Ïƒ=0.357\n    tensor[196] xâˆˆ[-1.656, 2.146] Î¼=-0.848 Ïƒ=0.372\n    tensor[196] xâˆˆ[-1.433, 2.215] Î¼=-0.784 Ïƒ=0.397\n    tensor[196] xâˆˆ[-1.279, 2.249] Î¼=-0.695 Ïƒ=0.486\n    tensor[196] xâˆˆ[-1.364, 2.249] Î¼=-0.637 Ïƒ=0.539\n    ...\n  tensor[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973 NaN!\n    tensor[196] xâˆˆ[-1.861, 2.411] Î¼=-0.529 Ïƒ=0.556\n    tensor[196] xâˆˆ[-1.826, 2.359] Î¼=-0.562 Ïƒ=0.473\n    tensor[196] xâˆˆ[-1.756, 2.376] Î¼=-0.622 Ïƒ=0.459 NaN!\n    tensor[196] xâˆˆ[-1.633, 2.429] Î¼=-0.664 Ïƒ=0.430\n    tensor[196] xâˆˆ[-1.651, 2.376] Î¼=-0.669 Ïƒ=0.399\n    tensor[196] xâˆˆ[-1.633, 2.376] Î¼=-0.701 Ïƒ=0.391\n    tensor[196] xâˆˆ[-1.563, 2.429] Î¼=-0.670 Ïƒ=0.380\n    tensor[196] xâˆˆ[-1.475, 2.429] Î¼=-0.616 Ïƒ=0.386\n    tensor[196] xâˆˆ[-1.511, 2.429] Î¼=-0.593 Ïƒ=0.399\n    ...\n  tensor[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178\n    tensor[196] xâˆˆ[-1.717, 2.396] Î¼=-0.982 Ïƒ=0.350\n    tensor[196] xâˆˆ[-1.752, 2.326] Î¼=-1.034 Ïƒ=0.314\n    tensor[196] xâˆˆ[-1.648, 2.379] Î¼=-1.086 Ïƒ=0.314\n    tensor[196] xâˆˆ[-1.630, 2.466] Î¼=-1.121 Ïƒ=0.305\n    tensor[196] xâˆˆ[-1.717, 2.448] Î¼=-1.120 Ïƒ=0.302\n    tensor[196] xâˆˆ[-1.717, 2.431] Î¼=-1.166 Ïƒ=0.314\n    tensor[196] xâˆˆ[-1.560, 2.448] Î¼=-1.124 Ïƒ=0.326\n    tensor[196] xâˆˆ[-1.421, 2.431] Î¼=-1.064 Ïƒ=0.383\n    tensor[196] xâˆˆ[-1.526, 2.396] Î¼=-1.047 Ïƒ=0.417\n    ...\n\n\n\n\nCUDA memory is not leaked\n\ndef memstats():\n    allocated = int(torch.cuda.memory_allocated() // (1024*1024))\n    max_allocated = int(torch.cuda.max_memory_allocated() // (1024*1024))\n    return f\"Allocated: {allocated} MB, Max: {max_allocated} Mb\"\n\nif torch.cuda.is_available():\n    cudamem = torch.cuda.memory_allocated()\n    print(f\"before allocation: {memstats()}\")\n    numbers = torch.randn((3, 1024, 1024), device=\"cuda\") # 12Mb image\n    torch.cuda.synchronize()\n\n    print(f\"after allocation: {memstats()}\")\n    # Note, the return value of lovely() is not a string, but a\n    # StrProxy that holds reference to 'numbers'. You have to del\n    # the references to it, but once it's gone, the reference to\n    # the tensor is gone too.\n    display(lovely(numbers) )\n    print(f\"after repr: {memstats()}\")\n    \n    del numbers\n    # torch.cuda.memory.empty_cache()\n\n    print(f\"after cleanup: {memstats()}\")\n    test_eq(cudamem >= torch.cuda.memory_allocated(), True)\n\nbefore allocation: Allocated: 0 MB, Max: 0 Mb\nafter allocation: Allocated: 12 MB, Max: 12 Mb\n\n\ntensor[3, 1024, 1024] n=3145728 xâˆˆ[-5.325, 5.150] Î¼=-0.000 Ïƒ=0.999 cuda:0\n\n\nafter repr: Allocated: 12 MB, Max: 12 Mb\nafter cleanup: Allocated: 0 MB, Max: 12 Mb\n\n\n\n# We don't really supposed complex numbers yet\nc = torch.randn(5, dtype=torch.complex64)\nlovely(c)\n\ntensor([-0.4011-0.4035j,  1.1300+0.0788j, -0.0277+0.9978j, -0.4636+0.6064j, -1.1505-0.9865j])"
  },
  {
    "objectID": "repr_rgb.html",
    "href": "repr_rgb.html",
    "title": "ðŸ–Œï¸ View as RGB images",
    "section": "",
    "text": "source\n\nrgb\n\n rgb (t:torch.Tensor, denorm=None, cl=False, gutter_px=3, frame_px=1,\n      scale=1, view_width=966)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nTensor\n\nTensor to display. [[â€¦], C,H,W] or [[â€¦], H,W,C]\n\n\ndenorm\nNoneType\nNone\nReverse per-channel normalizatoin\n\n\ncl\nbool\nFalse\nChannel-last\n\n\ngutter_px\nint\n3\nIf more than one tensor -> tile with this gutter width\n\n\nframe_px\nint\n1\nIf more than one tensor -> tile with this frame width\n\n\nscale\nint\n1\n\n\n\nview_width\nint\n966\ntarger width of the image\n\n\n\n\nrgb(image)\n\n\n\n\n\nrgb(image, scale=2)\n\n\n\n\n\ntwo_images = torch.stack([image]*2)\ntwo_images\n\ntensor[2, 3, 196, 196] n=230496 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073\n\n\n\nin_stats = (    (0.485, 0.456, 0.406),  # Mean\n                (0.229, 0.224, 0.225) ) # std\nrgb(two_images, denorm=in_stats)\n\n\n\n\n\n# Make 8 images with progressively higher brightness and stack them 2x2x2.\neight_images = (torch.stack([image]*8) + torch.linspace(-2, 2, 8)[:,None,None,None])\neight_images = (eight_images\n                    .mul(torch.tensor(in_stats[1])[:,None,None])\n                    .add(torch.tensor(in_stats[0])[:,None,None])\n                    .clamp(0,1)\n                    .view(2,2,2,3,196,196)\n)\neight_images\n\ntensor[2, 2, 2, 3, 196, 196] n=921984 xâˆˆ[0., 1.000] Î¼=0.382 Ïƒ=0.319\n\n\n\nrgb(eight_images)\n\n\n\n\n\n# You can do channel-last too:\nrgb(image.permute(1, 2, 0), cl=True)"
  },
  {
    "objectID": "repr_chans.html",
    "href": "repr_chans.html",
    "title": "ðŸ“º View channels",
    "section": "",
    "text": "source\n\nchans\n\n chans (t:torch.Tensor, cmap='twilight', cm_below='blue', cm_above='red',\n        cm_ninf='cyan', cm_pinf='fuchsia', cm_nan='yellow',\n        view_width=966, gutter_px=3, frame_px=1, scale=1, cl=False)\n\nMap tensor values to colors. RGB[A] color is added as channel-last\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nTensor\n\nInput, shape=([â€¦], H, W)\n\n\ncmap\nstr\ntwilight\nUse matplotlib colormap by this name\n\n\ncm_below\nstr\nblue\nColor for values below -1\n\n\ncm_above\nstr\nred\nColor for values above 1\n\n\ncm_ninf\nstr\ncyan\nColor for -inf values\n\n\ncm_pinf\nstr\nfuchsia\nColor for +inf values\n\n\ncm_nan\nstr\nyellow\nColor for NaN values\n\n\nview_width\nint\n966\nTry to produce an image at most this wide\n\n\ngutter_px\nint\n3\nDraw write gutters when tiling the images\n\n\nframe_px\nint\n1\nDraw black frame around each image\n\n\nscale\nint\n1\n\n\n\ncl\nbool\nFalse\n\n\n\n\n\nin_stats = ( (0.485, 0.456, 0.406), (0.229, 0.224, 0.225) )\n\nimage = torch.load(\"mysteryman.pt\")\nimage = (image * torch.tensor(in_stats[1])[:,None,None])\nimage += torch.tensor(in_stats[0])[:,None,None]\n\nimage.rgb\n\n\n\n\n\nchans(image)\n\n\n\n\n\n# In R\nimage[0,0:32,32:64] = -1.1 # Below min\nimage[0,0:32,96:128] = 1.1 # Above max\n# In G\nimage[1,0:32,64:96] = float(\"nan\")\n# In B\nimage[2,0:32,0:32] = float(\"-inf\")\nimage[2,0:32,128:128+32] = float(\"+inf\")\n\nchans(image, cmap=\"viridis\", cm_below=\"black\", cm_above=\"white\")\n\n\n\n\n\n# 4 images, stacked 2x2\nchans(torch.stack([image]*4).view(2,2,3,196,196))\n\n\n\n\n\ntry:\n    chans(torch.tensor([]).view((0,0,0)))\nexcept AssertionError as e:\n    test_eq(e.args[0], \"Expecting non-empty input, got shape=((0, 0, 0, 3))\")\nelse:\n    raise AssertionError(\"Expected AssertionError, but got nothing\")"
  },
  {
    "objectID": "utils.config.html",
    "href": "utils.config.html",
    "title": "ðŸ¤” Config",
    "section": "",
    "text": "Type\nDefault\nDetails\n\n\n\n\nprecision\nint\n3\nDigits after .\n\n\nthreshold_max\nint\n3\n.abs() larger than 1e3 -> Sci mode\n\n\nthreshold_min\nint\n-4\n.abs() smaller that 1e-4 -> Sci mode\n\n\nsci_mode\nNoneType\nNone\nSci mode (2.3e4). None=auto\n\n\nindent\nint\n2\nIndent for .deeper()\n\n\ncolor\nbool\nTrue\nANSI colors in text\n\n\ndeeper_width\nint\n9\nFor .deeper, width per level\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "utils.config.html#examples",
    "href": "utils.config.html#examples",
    "title": "ðŸ¤” Config",
    "section": "Examples",
    "text": "Examples\n\nimport torch\nfrom lovely_tensors import lovely, set_config, get_config, config\n\n\nPrecision\n\n_config\n\nnamespace(precision=3,\n          threshold_max=3,\n          threshold_min=-4,\n          sci_mode=None,\n          indent=2,\n          color=True,\n          deeper_width=9)\n\n\n\nset_config(precision=5, )\nlovely(torch.tensor([1., 2, float(\"nan\")]))\n\n\ntensor[3] Î¼=1.50000 Ïƒ=0.70711 NaN! [1.00000, 2.00000, nan]\n\n\n\n\n\nScientific mode\n\nset_config(sci_mode=True) # Force always on\nlovely(torch.tensor([1., 2, float(\"nan\")]))\n\n\ntensor[3] Î¼=1.50000e+00 Ïƒ=7.07107e-01 NaN! [1.00000e+00, 2.00000e+00, nan]\n\n\n\n\n\nColor on/off\n\nset_config(color=False) # Force always off\nlovely(torch.tensor([1., 2, float(\"nan\")]))\n\ntensor[3] Î¼=1.50000e+00 Ïƒ=7.07107e-01 NaN! [1.00000e+00, 2.00000e+00, nan]\n\n\n\ntest_eq(str(lovely(torch.tensor([1., 2, float(\"nan\")]))),\n        'tensor[3] Î¼=1.50000e+00 Ïƒ=7.07107e-01 NaN! [1.00000e+00, 2.00000e+00, nan]')\n\n\n\nControl .deeper\n\nset_config(deeper_width=2) \nimage = torch.load(\"mysteryman.pt\")\nimage[1,100,100] = float('nan')\n\nlovely(image, depth=2)\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-2.11790e+00, 2.64000e+00] Î¼=-3.88310e-01 Ïƒ=1.07319e+00 NaN!\n  tensor[196, 196] n=38416 xâˆˆ[-2.11790e+00, 2.24891e+00] Î¼=-3.24352e-01 Ïƒ=1.03588e+00\n    tensor[196] xâˆˆ[-1.91241e+00, 2.24891e+00] Î¼=-6.73483e-01 Ïƒ=5.21962e-01\n    tensor[196] xâˆˆ[-1.86103e+00, 2.16328e+00] Î¼=-7.38488e-01 Ïƒ=4.18080e-01\n    ...\n  tensor[196, 196] n=38416 xâˆˆ[-1.96569e+00, 2.42857e+00] Î¼=-2.73903e-01 Ïƒ=9.72665e-01 NaN!\n    tensor[196] xâˆˆ[-1.86064e+00, 2.41106e+00] Î¼=-5.28772e-01 Ïƒ=5.55960e-01\n    tensor[196] xâˆˆ[-1.82563e+00, 2.35854e+00] Î¼=-5.61732e-01 Ïƒ=4.72772e-01\n    ...\n  ...\n\n\n\ntest_eq(len(str(lovely(image, depth=2))), 591)\n\n\n\nReser to defaults\n\nset_config(precision=None, sci_mode=None, color=None, deeper_width=None)\nlovely(torch.tensor([1., 2, float(\"nan\")]))\n\n\ntensor[3] Î¼=1.500 Ïƒ=0.707 NaN! [1.000, 2.000, nan]\n\n\n\n\ntest_eq(str(lovely(torch.tensor([1., 2, float(\"nan\")]))),\n    'tensor[3] Î¼=1.500 Ïƒ=0.707 \\x1b[31mNaN!\\x1b[0m [1.000, 2.000, nan]')\n\n\n\nContext manager\n\ndisplay(lovely(torch.tensor([1., 2, torch.nan])))\nwith config(sci_mode=True, color=False):\n    display(lovely(torch.tensor([1., 2, torch.nan])))\ndisplay(lovely(torch.tensor([1., 2, torch.nan])))\n\n\ntensor[3] Î¼=1.500 Ïƒ=0.707 NaN! [1.000, 2.000, nan]\n\n\n\ntensor[3] Î¼=1.500e+00 Ïƒ=7.071e-01 NaN! [1.000e+00, 2.000e+00, nan]\n\n\n\ntensor[3] Î¼=1.500 Ïƒ=0.707 NaN! [1.000, 2.000, nan]"
  },
  {
    "objectID": "patch.html",
    "href": "patch.html",
    "title": "ðŸ™‰ Monkey-patching",
    "section": "",
    "text": "source\n\nmonkey_patch\n\n monkey_patch (cls=<class 'torch.Tensor'>)\n\nMonkey-patch lovely features into cls\n\nmonkey_patch()\n\n\nimage = torch.load(\"mysteryman.pt\")\n\n\nspicy = image.flatten()[:12].clone()\n\nspicy[0] *= 10000\nspicy[1] /= 10000\nspicy[2] = float('inf')\nspicy[3] = float('-inf')\nspicy[4] = float('nan')\n\nspicy = spicy.reshape((2,6))\nspicy\n\n\ntensor[2, 6] n=12 xâˆˆ[-3.541e+03, -3.369e-05] Î¼=-393.776 Ïƒ=1.180e+03 +Inf! -Inf! NaN!\n\n\n\n\nspicy.v\n\n\ntensor[2, 6] n=12 xâˆˆ[-3.541e+03, -3.369e-05] Î¼=-393.776 Ïƒ=1.180e+03 +Inf! -Inf! NaN!\ntensor([[-3.5405e+03, -3.3693e-05,         inf,        -inf,         nan,\n         -4.0543e-01],\n        [-4.2255e-01, -4.9105e-01, -5.0818e-01, -5.5955e-01, -5.4243e-01,\n         -5.0818e-01]])\n\n\n\n\nspicy.p\n\ntensor([[-3.5405e+03, -3.3693e-05,         inf,        -inf,         nan,\n         -4.0543e-01],\n        [-4.2255e-01, -4.9105e-01, -5.0818e-01, -5.5955e-01, -5.4243e-01,\n         -5.0818e-01]])\n\n\n\nimage.deeper\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073\n  tensor[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036\n  tensor[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973\n  tensor[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178\n\n\n\nimage[:3,:3,:5].deeper(depth=2)\n\ntensor[3, 3, 5] n=45 xâˆˆ[-1.316, -0.197] Î¼=-0.593 Ïƒ=0.306\n  tensor[3, 5] n=15 xâˆˆ[-0.765, -0.337] Î¼=-0.492 Ïƒ=0.124\n    tensor[5] xâˆˆ[-0.440, -0.337] Î¼=-0.385 Ïƒ=0.041 [-0.354, -0.337, -0.405, -0.440, -0.388]\n    tensor[5] xâˆˆ[-0.662, -0.405] Î¼=-0.512 Ïƒ=0.108 [-0.405, -0.423, -0.491, -0.577, -0.662]\n    tensor[5] xâˆˆ[-0.765, -0.474] Î¼=-0.580 Ïƒ=0.125 [-0.474, -0.474, -0.542, -0.645, -0.765]\n  tensor[3, 5] n=15 xâˆˆ[-0.513, -0.197] Î¼=-0.321 Ïƒ=0.099\n    tensor[5] xâˆˆ[-0.303, -0.197] Î¼=-0.243 Ïƒ=0.055 [-0.197, -0.197, -0.303, -0.303, -0.215]\n    tensor[5] xâˆˆ[-0.408, -0.232] Î¼=-0.327 Ïƒ=0.084 [-0.250, -0.232, -0.338, -0.408, -0.408]\n    tensor[5] xâˆˆ[-0.513, -0.285] Î¼=-0.394 Ïƒ=0.102 [-0.303, -0.285, -0.390, -0.478, -0.513]\n  tensor[3, 5] n=15 xâˆˆ[-1.316, -0.672] Î¼=-0.964 Ïƒ=0.176\n    tensor[5] xâˆˆ[-0.985, -0.672] Î¼=-0.846 Ïƒ=0.123 [-0.672, -0.985, -0.881, -0.776, -0.916]\n    tensor[5] xâˆˆ[-1.212, -0.724] Î¼=-0.989 Ïƒ=0.179 [-0.724, -1.072, -0.968, -0.968, -1.212]\n    tensor[5] xâˆˆ[-1.316, -0.828] Î¼=-1.058 Ïƒ=0.179 [-0.828, -1.125, -1.020, -1.003, -1.316]\n\n\n\nimage.rgb\n\n\n\n\n\nin_stats = ( (0.485, 0.456, 0.406),     # mean \n             (0.229, 0.224, 0.225) )    # std\nimage.rgb(in_stats)\n\n\n\n\n\nmean = torch.tensor(in_stats[0])[:,None,None]\nstd = torch.tensor(in_stats[1])[:,None,None]\n\n(image*std + mean).chans # all pixels in [0, 1] range\n\n\n\n\n\n(image*0.3+0.5) # Slightly outside of [0, 1] range\n\ntensor[3, 196, 196] n=115248 xâˆˆ[-0.135, 1.292] Î¼=0.384 Ïƒ=0.322\n\n\n\n(image*0.3+0.5).chans # shows clipping (bright blue/red)\n\n\n\n\n\nimage.plt\n\n\n\n\n\nimage.plt(center=\"mean\")\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 2))\nplt.close(fig)\nimage.plt(ax=ax)\nfig"
  }
]